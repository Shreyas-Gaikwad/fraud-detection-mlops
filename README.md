# Fraud Detection MLOps System ğŸš¨

An end-to-end, production-grade fraud detection system built with a strong focus on **decision optimization, explainability, and deployment**.
This project goes beyond model training to address real-world challenges such as class imbalance, threshold selection, trainingâ€“serving skew, and model governance.

ğŸ”— **Live Demo (Swagger UI):**
[https://fraud-detection-mlops-f98a.onrender.com/docs/](https://fraud-detection-mlops-f98a.onrender.com/docs/)

<img width="1577" height="812" alt="image" src="https://github.com/user-attachments/assets/6b75a66b-e3f7-44d8-be8c-f1d327ad3daa" />

---

## ğŸ” Problem Statement

Credit card fraud detection is a **highly imbalanced classification problem** where:

* Fraud rates are extremely low
* False positives create customer friction
* Accuracy is a misleading metric

The goal is not just to build a classifier, but to design a **decision system** that:

* Maximizes fraud capture (recall)
* Controls false positives (FPR)
* Is explainable and deployable

---

## ğŸ§  Key Design Decisions

### 1ï¸âƒ£ Metric Discipline

* Used **PR-AUC** instead of accuracy or ROC-AUC
* Explicitly analyzed metric behavior under extreme class imbalance

### 2ï¸âƒ£ Cost-Aware Learning

* Simulated realistic fraud rates (1% / 5%)
* Used class-weighted models to reflect asymmetric costs

### 3ï¸âƒ£ Decision Threshold Optimization

* Rejected the default 0.5 probability cutoff
* Selected an operational threshold (**0.006**) based on:

  * Maximum recall
  * False Positive Rate â‰¤ 0.1%

ğŸ“Œ *This threshold is treated as a first-class artifact, not a magic constant.*

### 4ï¸âƒ£ Trainingâ€“Serving Consistency

* Identified and fixed feature schema mismatch (`id` leakage)
* Ensured deterministic feature ordering and schema alignment
* Prevented silent inference failures common in production ML systems

---

## ğŸ—ï¸ System Architecture

```
Raw Data
   â”‚
   â–¼
Data Preprocessing & Imbalance Simulation
   â”‚
   â–¼
Model Training (Logistic Regression, XGBoost)
   â”‚
   â–¼
MLflow Experiment Tracking
   â”‚
   â–¼
Threshold Optimization (Recall vs FPR)
   â”‚
   â–¼
FastAPI Inference Service
   â”‚
   â”œâ”€â”€ /predict  â†’ Probability-based fraud decision
   â””â”€â”€ /explain  â†’ SHAP-based feature attribution
   â”‚
   â–¼
Dockerized Deployment â†’ Render (Cloud)
```

---

## ğŸ¤– Models Used

| Model               | Purpose                              |
| ------------------- | ------------------------------------ |
| Logistic Regression | Interpretable baseline               |
| XGBoost             | High-performance fraud ranking model |

XGBoost was selected for deployment due to superior **PR-AUC** and ranking performance.

---

## ğŸ“Š Explainability with SHAP

To support governance and human review, the system exposes a dedicated `/explain` endpoint using **SHAP**.

Example response:

```json
{
  "fraud_probability": 0.999284,
  "decision": true,
  "top_contributing_features": {
    "V14": 3.381843,
    "V8": -1.714934,
    "V4": 1.412652
  }
}
```

This enables:

* Feature-level attribution per transaction
* Regulatory explainability
* Human-in-the-loop decision support

---

## ğŸš€ API Endpoints

| Endpoint   | Description                  |
| ---------- | ---------------------------- |
| `/health`  | Service health check         |
| `/predict` | Fraud probability + decision |
| `/explain` | SHAP-based explanation       |

Swagger UI:
ğŸ‘‰ [https://fraud-detection-mlops-f98a.onrender.com/docs/](https://fraud-detection-mlops-f98a.onrender.com/docs/)

---

## ğŸ³ Deployment & MLOps

* FastAPI for inference
* Dockerized service
* Deployed on **Render**
* Cloudflare-proxied public endpoint
* Structured logging for monitoring and alert-rate tracking

<img width="1580" height="761" alt="image" src="https://github.com/user-attachments/assets/c1a2a1e5-fbce-4aa2-a620-1de87155c664" />

This demonstrates full **model-to-production lifecycle ownership**.

---

## ğŸ§ª Testing & Validation

* Tested locally via Swagger UI
* Verified via `curl`
* Validated using known fraud and non-fraud samples
* Confirmed behavior under low-risk and high-risk inputs

---

## ğŸ“ Project Structure

```
fraud-detection-mlops/
â”œâ”€â”€ api/               # FastAPI application
â”œâ”€â”€ src/               # Training, evaluation, utilities
â”œâ”€â”€ artifacts/         # Trained model & threshold
â”œâ”€â”€ docker/            # Dockerfile
â”œâ”€â”€ notebooks/         # Analysis & threshold tuning
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§¾ Key Learnings

* Fraud detection is a **decision problem**, not just a classification task
* Threshold selection matters more than raw accuracy
* Trainingâ€“serving skew is a real production risk
* Explainability is essential for trust and governance
* Deployment completes the ML lifecycle

---

## ğŸ”® Future Improvements

* Batch prediction endpoint
* Drift detection on input distributions
* CI pipeline for automated builds
* Cloud deployment on AWS ECS

---

## ğŸ‘¤ Author

Built by **[Shreyas Gaikwad]**
End-to-End Machine Learning & MLOps

